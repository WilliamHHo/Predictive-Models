{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33c5b0c-d0f1-40c0-b794-3b3471ac73d2",
   "metadata": {
    "id": "e33c5b0c-d0f1-40c0-b794-3b3471ac73d2"
   },
   "source": [
    "# Applied Machine Learning - Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4651888-484b-42a0-95e1-d273e5069205",
   "metadata": {
    "id": "a4651888-484b-42a0-95e1-d273e5069205"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086914c2-5de1-414a-8770-23bef9f312d0",
   "metadata": {
    "id": "086914c2-5de1-414a-8770-23bef9f312d0"
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd",
   "metadata": {
    "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd"
   },
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 16, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings [work well on a variety of text classification tasks](http://www.lrec-conf.org/proceedings/lrec2018/pdf/721.pdf). These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads pre-trained word vectors trained on Wikipedia. (The vectors are created using an algorithm called GloVe.) To run the code, you'll need `gensim` package for that in your cpsc330 conda environment, which you can install as follows. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
    "outputId": "3c1e4408-00c9-4eba-aa2a-66aff9f55d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467",
   "metadata": {
    "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467"
   },
   "outputs": [],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ec38c4-ce89-4372-b015-035f4d682132",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76ec38c4-ce89-4372-b015-035f4d682132",
    "outputId": "33baf820-3750-4069-8076-0640aab3e34f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors.index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78dafb-f712-447a-b870-1fac6c249e5f",
   "metadata": {
    "id": "8c78dafb-f712-447a-b870-1fac6c249e5f"
   },
   "source": [
    "There are 400,000 word vectors in these pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3",
   "metadata": {
    "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119fb78-d2be-4ccf-8c8d-31026563e072",
   "metadata": {
    "id": "8119fb78-d2be-4ccf-8c8d-31026563e072"
   },
   "source": [
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "\n",
    "Now that we have GloVe Wiki vectors (`glove_wiki_vectors`) loaded, let's explore the word vectors. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of the model.\n",
    "2. Do the similarities make sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadd1aa6-6bb8-48d7-a959-691e19d411ec",
   "metadata": {
    "id": "aadd1aa6-6bb8-48d7-a959-691e19d411ec"
   },
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "-JSAQB94s57i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JSAQB94s57i",
    "outputId": "860b704e-a2f1-4d08-cae2-5f56271b9f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between coast and shore is 0.70002717\n",
      "The similarity between clothes and closet is 0.54627603\n",
      "The similarity between old and new is 0.6432488\n",
      "The similarity between smart and intelligent is 0.7552733\n",
      "The similarity between dog and cat is 0.8798075\n",
      "The similarity between tree and lawyer is 0.076719455\n"
     ]
    }
   ],
   "source": [
    "for index, tuple in enumerate(word_pairs):\n",
    "  word1 = tuple[0]\n",
    "  word2 = tuple[1]\n",
    "  similarity = glove_wiki_vectors.similarity(word1, word2)\n",
    "  print('The similarity between', word1, 'and', word2, 'is', similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ca2a9-eba8-472e-a49b-9d0561d96846",
   "metadata": {
    "id": "165ca2a9-eba8-472e-a49b-9d0561d96846"
   },
   "source": [
    "The similarities make sense. The lowest value is trees and lawyer, because most of the time, they don't have much to do with eachother. Dog and cats are the most common domestic pets, so it makes sense that they have a high correlation. The rest also make sense to have a fairly high correlation as smart and intelligent are synonyms, coast and shore are usually right next to eachother, and old and now are opposites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0331ffe-bf58-4198-bd1e-3b62a5d34319",
   "metadata": {
    "id": "b0331ffe-bf58-4198-bd1e-3b62a5d34319"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d528120-c1ff-4203-82b8-404b62ba6bb0",
   "metadata": {
    "id": "4d528120-c1ff-4203-82b8-404b62ba6bb0"
   },
   "source": [
    "### 1.2 Bias in embeddings\n",
    "\n",
    "**Your tasks:**\n",
    "1. Pre-trained word embedding model may output an analogy that reinforced a gender stereotype. Give an example of how using such a model could cause harm in the real world.\n",
    "2. Here we are using pre-trained embeddings which are built using Wikipedia data. Explore whether there are any worrisome biases present in these embeddings or not by trying out some examples. You can use the following two methods or other methods of your choice to explore what kind of stereotypes and biases are encoded in these embeddings. \n",
    "    - You can use the `analogy` function below which gives words analogies. \n",
    "    - You can also use [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods. (An example is shown below.)   \n",
    "3. Discuss your observations. Do you observe the gender stereotype we observed in class in these embeddings?\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59f7ef2b-d6b4-4338-a153-77fd4fc9847e",
   "metadata": {
    "id": "59f7ef2b-d6b4-4338-a153-77fd4fc9847e"
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wWxJ9h3ONAf-",
   "metadata": {
    "id": "wWxJ9h3ONAf-"
   },
   "source": [
    "Using a model based on gender could potentially be discrimatory towards a specific gender. For example, computer science is generally considered a male dominated field. Using a model that takes this into account in whether someone gets accepted to a course or not may lower the chances of a female person from getting into the field reinforcing the gender stereotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "CKh32ixwuumy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKh32ixwuumy",
    "outputId": "35815352-c9d6-46bb-e63e-4f0df1da348a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46977922"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"man\", \"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6QDW67NOuvjD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QDW67NOuvjD",
    "outputId": "083645e5-5cf9-4c47-8c40-f398a306a201"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3507729"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"woman\", \"rich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opyXhAgTMFeF",
   "metadata": {
    "id": "opyXhAgTMFeF"
   },
   "source": [
    "Between men and women and their correlation with the word rich, woman are about 25% lower in their similarity. While this isn't really desirable, this makes sense considering the gender pay gap. Acording to the Canadian Women's Foundation (https://canadianwomen.org/the-facts/the-gender-pay-gap/), women make 71% of what men make. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3oYKvC3urpC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "d3oYKvC3urpC",
    "outputId": "e7580894-153e-4faf-d393-59b5db5e498f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : househusband :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d21d5ca2-b183-4652-9d5a-c336c8b9f4ab\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gold-digger</td>\n",
       "      <td>0.817556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tough-as-nails</td>\n",
       "      <td>0.783552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chiropodist</td>\n",
       "      <td>0.772597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love-struck</td>\n",
       "      <td>0.761878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wet-nurse</td>\n",
       "      <td>0.758541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>golddigger</td>\n",
       "      <td>0.757900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>strong-minded</td>\n",
       "      <td>0.757849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>self-possessed</td>\n",
       "      <td>0.756513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>elisaveta</td>\n",
       "      <td>0.751576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scotswoman</td>\n",
       "      <td>0.750467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d21d5ca2-b183-4652-9d5a-c336c8b9f4ab')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d21d5ca2-b183-4652-9d5a-c336c8b9f4ab button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d21d5ca2-b183-4652-9d5a-c336c8b9f4ab');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     Analogy word     Score\n",
       "0  gold-digger     0.817556\n",
       "1  tough-as-nails  0.783552\n",
       "2  chiropodist     0.772597\n",
       "3  love-struck     0.761878\n",
       "4  wet-nurse       0.758541\n",
       "5  golddigger      0.757900\n",
       "6  strong-minded   0.757849\n",
       "7  self-possessed  0.756513\n",
       "8  elisaveta       0.751576\n",
       "9  scotswoman      0.750467"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"househusband\", \"woman\", glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "_Qt3YHo-us1U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "_Qt3YHo-us1U",
    "outputId": "02054fa8-9ae3-4333-fe76-966d7095b215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman : housewife :: man : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b1c8f572-4c16-47b3-9c5a-ce31b94e3044\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homemaker</td>\n",
       "      <td>0.618261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loner</td>\n",
       "      <td>0.581096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>schoolteacher</td>\n",
       "      <td>0.579345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slacker</td>\n",
       "      <td>0.566048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dad</td>\n",
       "      <td>0.550712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>thug</td>\n",
       "      <td>0.535593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shopkeeper</td>\n",
       "      <td>0.526345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>workaholic</td>\n",
       "      <td>0.524418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mom</td>\n",
       "      <td>0.523697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>old</td>\n",
       "      <td>0.518858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1c8f572-4c16-47b3-9c5a-ce31b94e3044')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b1c8f572-4c16-47b3-9c5a-ce31b94e3044 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b1c8f572-4c16-47b3-9c5a-ce31b94e3044');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0  homemaker      0.618261\n",
       "1  loner          0.581096\n",
       "2  schoolteacher  0.579345\n",
       "3  slacker        0.566048\n",
       "4  dad            0.550712\n",
       "5  thug           0.535593\n",
       "6  shopkeeper     0.526345\n",
       "7  workaholic     0.524418\n",
       "8  mom            0.523697\n",
       "9  old            0.518858"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"woman\", \"housewife\", \"man\", glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf431d-a56d-4d78-b381-eee21bceb049",
   "metadata": {
    "id": "0bbf431d-a56d-4d78-b381-eee21bceb049"
   },
   "source": [
    "Equating a woman to a housewife and a man to a homemaker makes sense, but equating a househusband to a gold-digger is definitely worrisome. The score is 0.81 which means that the association is fairly strong. These embeddings definitely have some gender stereotypes, but generally are pretty accurate. Generally, you actively have to be looking for worrisome gender stereotypes to actually find one. Most times there aren't any glaring flaws."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f04b87-5fa0-4eb4-bb50-cb21aa7ffd1c",
   "metadata": {
    "id": "19f04b87-5fa0-4eb4-bb50-cb21aa7ffd1c"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ccd3e-25c1-413d-b079-043fb2949090",
   "metadata": {
    "id": "9e6ccd3e-25c1-413d-b079-043fb2949090"
   },
   "source": [
    "### 1.3 Representation of all words in English\n",
    "\n",
    "**Your tasks:**\n",
    "1. The vocabulary size of Wikipedia embeddings is quite large. Do you think it contains **all** words in English language? What would happen if you try to get a word vector that's unlikely to be present in the vocabulary (e.g., the word \"cpsc330\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589eeae-c081-4042-8cdc-7dbc85a2c400",
   "metadata": {
    "id": "d589eeae-c081-4042-8cdc-7dbc85a2c400"
   },
   "source": [
    "No, it probably doesn't. There may be a lot of slang. onomatopoeias, or informal words that the wikipedia embedding may be missing. If you try to get a word vector that's unlikely in the vocabulary, you likely won't be able to get any results, because the embedding doesn't have any info to base that word vector on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a0fbf-3a9c-42b3-b9cc-5dc474bc2804",
   "metadata": {
    "id": "464a0fbf-3a9c-42b3-b9cc-5dc474bc2804"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3cd04-30c9-43f4-9b07-6443ab4ecd7d",
   "metadata": {
    "id": "f1d3cd04-30c9-43f4-9b07-6443ab4ecd7d"
   },
   "source": [
    "### 1.4 Classification with pre-trained embeddings\n",
    "\n",
    "We saw that you can conveniently get word vectors with `spaCy` with `en_core_web_md` model. In this exercise, you'll use word embeddings in multi-class text classification task. We will use [HappyDB](https://www.kaggle.com/ritresearch/happydb) corpus which contains about 100,000 happy moments classified into 7 categories: *affection, exercise, bonding, nature, leisure, achievement, enjoy_the_moment*. The data was crowd-sourced via [Amazon Mechanical Turk](https://www.mturk.com/). The ground truth label is not available for all examples, and in this lab, we'll only use the examples where ground truth is available (~15,000 examples). \n",
    "\n",
    "- Download the data from [here](https://www.kaggle.com/ritresearch/happydb).\n",
    "- Unzip the file and copy it in the lab directory.\n",
    "\n",
    "We will be using spaCy in this exercise. If you do not have spaCy in your course environment, here is how you can install it.  \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c conda-forge spacy\n",
    "```\n",
    "\n",
    "- You also need to download the language model which contains all the pre-trained models. For that run the following in your course `conda` environment. \n",
    "```\n",
    "python -m spacy download en_core_web_md\n",
    "```\n",
    "\n",
    "The code below reads the data CSV (assuming that it's present in the current directory as *cleaned_hm.csv*),  cleans it up a bit, and splits it into train and test splits. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Train logistic regression with bag-of-words features and show classification report on the test set. \n",
    "2. Train logistic regression with average embedding representation extracted using spaCy and show classification report on the test set.  \n",
    "3. Discuss your results. Which model is performing well. Which model would be more interpretable?  \n",
    "4. Are you observing any benefits of transfer learning here? Briefly discuss. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7b35845-7976-4cda-b798-0c0700868fea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 840
    },
    "id": "b7b35845-7976-4cda-b798-0c0700868fea",
    "outputId": "55f9be6d-eda0-4df1-efa4-a4957fc6792c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f3dec59f-fca9-4be7-aa71-d76551802848\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>original_hm</th>\n",
       "      <th>cleaned_hm</th>\n",
       "      <th>modified</th>\n",
       "      <th>num_sentence</th>\n",
       "      <th>ground_truth_category</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hmid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>206</td>\n",
       "      <td>24h</td>\n",
       "      <td>We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.</td>\n",
       "      <td>We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>bonding</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27678</th>\n",
       "      <td>45</td>\n",
       "      <td>24h</td>\n",
       "      <td>I meditated last night.</td>\n",
       "      <td>I meditated last night.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>leisure</td>\n",
       "      <td>leisure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27697</th>\n",
       "      <td>498</td>\n",
       "      <td>24h</td>\n",
       "      <td>My grandmother start to walk from the bed after a long time.</td>\n",
       "      <td>My grandmother start to walk from the bed after a long time.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>affection</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27705</th>\n",
       "      <td>5732</td>\n",
       "      <td>24h</td>\n",
       "      <td>I picked my daughter up from the airport and we have a fun and good conversation on the way home.</td>\n",
       "      <td>I picked my daughter up from the airport and we have a fun and good conversation on the way home.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>bonding</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27715</th>\n",
       "      <td>2272</td>\n",
       "      <td>24h</td>\n",
       "      <td>when i received flowers from my best friend</td>\n",
       "      <td>when i received flowers from my best friend</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>bonding</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3dec59f-fca9-4be7-aa71-d76551802848')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f3dec59f-fca9-4be7-aa71-d76551802848 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f3dec59f-fca9-4be7-aa71-d76551802848');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "        wid reflection_period  \\\n",
       "hmid                            \n",
       "27676  206   24h                \n",
       "27678  45    24h                \n",
       "27697  498   24h                \n",
       "27705  5732  24h                \n",
       "27715  2272  24h                \n",
       "\n",
       "                                                                                                                              original_hm  \\\n",
       "hmid                                                                                                                                        \n",
       "27676  We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.   \n",
       "27678  I meditated last night.                                                                                                              \n",
       "27697  My grandmother start to walk from the bed after a long time.                                                                         \n",
       "27705  I picked my daughter up from the airport and we have a fun and good conversation on the way home.                                    \n",
       "27715  when i received flowers from my best friend                                                                                          \n",
       "\n",
       "                                                                                                                               cleaned_hm  \\\n",
       "hmid                                                                                                                                        \n",
       "27676  We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.   \n",
       "27678  I meditated last night.                                                                                                              \n",
       "27697  My grandmother start to walk from the bed after a long time.                                                                         \n",
       "27705  I picked my daughter up from the airport and we have a fun and good conversation on the way home.                                    \n",
       "27715  when i received flowers from my best friend                                                                                          \n",
       "\n",
       "       modified  num_sentence ground_truth_category predicted_category  \n",
       "hmid                                                                    \n",
       "27676  True      2             bonding               bonding            \n",
       "27678  True      1             leisure               leisure            \n",
       "27697  True      1             affection             affection          \n",
       "27705  True      1             bonding               affection          \n",
       "27715  True      1             bonding               bonding            "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_hm.csv\", index_col=0)\n",
    "sample_df = df.dropna()\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "384fa13b-83a5-4e23-9280-c4e529937143",
   "metadata": {
    "id": "384fa13b-83a5-4e23-9280-c4e529937143"
   },
   "outputs": [],
   "source": [
    "sample_df = sample_df.rename(\n",
    "    columns={\"cleaned_hm\": \"moment\", \"ground_truth_category\": \"target\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de04d594-7174-409d-a9fa-c2fd738e2208",
   "metadata": {
    "id": "de04d594-7174-409d-a9fa-c2fd738e2208"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(sample_df, test_size=0.3, random_state=123)\n",
    "X_train, y_train = train_df[\"moment\"], train_df[\"target\"]\n",
    "X_test, y_test = test_df[\"moment\"], test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73f63a73-0d13-4276-9e79-7ad60575a198",
   "metadata": {
    "id": "73f63a73-0d13-4276-9e79-7ad60575a198"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hT_sxBdnvGLL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hT_sxBdnvGLL",
    "outputId": "8be87036-1017-4c8c-f817-9b7985988c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix shape: (9887, 8060)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pipe = make_pipeline(CountVectorizer(stop_words=\"english\"), LogisticRegression(max_iter=1000))\n",
    "pipe.named_steps[\"countvectorizer\"].fit(X_train)\n",
    "X_train_transformed = pipe.named_steps[\"countvectorizer\"].transform(X_train)\n",
    "print(\"Data matrix shape:\", X_train_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vhsUsqXivJpi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhsUsqXivJpi",
    "outputId": "b35fd3c2-cff3-4e8b-95d2-43d42b264206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     achievement       0.79      0.87      0.83      1302\n",
      "       affection       0.90      0.91      0.91      1423\n",
      "         bonding       0.91      0.85      0.88       492\n",
      "enjoy_the_moment       0.60      0.54      0.57       469\n",
      "        exercise       0.91      0.57      0.70        74\n",
      "         leisure       0.73      0.70      0.71       407\n",
      "          nature       0.73      0.46      0.57        71\n",
      "\n",
      "        accuracy                           0.82      4238\n",
      "       macro avg       0.80      0.70      0.74      4238\n",
      "    weighted avg       0.82      0.82      0.81      4238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train);\n",
    "lr_prediction = pipe.predict(X_test)\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test, lr_prediction\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dOSw0cX9vLXz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOSw0cX9vLXz",
    "outputId": "8b188a36-d309-4466-cb1a-cf77f63f3615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9887, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_train)])\n",
    "X_test_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_test)])\n",
    "X_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eQr1HBz6vMyr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQr1HBz6vMyr",
    "outputId": "4328a2d9-8f98-47dd-f467-7b98cf6f5964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     achievement       0.80      0.86      0.83      1302\n",
      "       affection       0.87      0.93      0.90      1423\n",
      "         bonding       0.84      0.76      0.80       492\n",
      "enjoy_the_moment       0.60      0.54      0.57       469\n",
      "        exercise       0.84      0.66      0.74        74\n",
      "         leisure       0.81      0.69      0.75       407\n",
      "          nature       0.80      0.68      0.73        71\n",
      "\n",
      "        accuracy                           0.81      4238\n",
      "       macro avg       0.80      0.73      0.76      4238\n",
      "    weighted avg       0.81      0.81      0.81      4238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(max_iter=1000)\n",
    "lgr.fit(X_train_embeddings, y_train)\n",
    "lr_prediction = lgr.predict(X_test_embeddings)\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test, lr_prediction\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f345014-d6cc-46e6-b964-4dd412f46a04",
   "metadata": {
    "id": "1f345014-d6cc-46e6-b964-4dd412f46a04"
   },
   "source": [
    "In this case, there is not much difference between the bag-of-words presentation and the average embedding with logistic regression. Most of the precision, recall and f1-score values for each categorization only differ by less than 0.5. The biggest difference by far is the recall score of nature where average embedding is much better by 0.20. Transfer learning is definitely benefitting this model because of the increase in predicting nature's score. While it loses some precision for most of the values, it also majorly increases the recall value for others. This makes it more interpretable and more reliable to use compared to the bag-of-words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a6ab6-62ef-4fdd-ba0d-5e7e920154a3",
   "metadata": {
    "id": "161a6ab6-62ef-4fdd-ba0d-5e7e920154a3"
   },
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
